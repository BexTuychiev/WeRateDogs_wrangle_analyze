{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wrangle_act notebook, I have performed several actions to clean three datasets.\n",
    "\n",
    "I created the first dataset, we_rd, from the WeRateDogs enhanced twitter archive. The dataset included almost 2500 records of tweets and their id, rating, dog stage, etc. columns. Then, using requests library, I downloaded the contents of tweets corresponding to the tweet_id column in the first dataset. I stored the like count and the number of retweets from a json file to a second dataset, tweets_selected. Final dataset included the results of neural network algorithm to identify the dog breeds. The data was given a direct access for those who could not get the Twitter API developer account.\n",
    "\n",
    "Overall, I have identified 9 quality and 7 tidiness issues. Most of the issues were in the we_rd dataset. Moreover, it was also very surprising to see so many tidiness issues. Two of the most notable tidiness issues were that dog stages were spread out in 4 different columns and had so many missing values. Even though, I could not fill out the missing values, I have successfully combined dog stages into one single column. The dataset also contained whether the tweet was a retweet or a just a reply to the original tweet. These informations were given in two columns. First, I extracted the ids of the tweets that were retweeted. Then, all the tweets  which were replies. Using these ids, I dropped all the corresponding tweets in all three datasets. Another change which was absolutely essential was that some of the ratings of the dogs were extremely high or extremely low. I have analyzed the original channel of WeRateDogs and found out that their highest ever record rating was 15/10. So for all the numerators and denominators there were higher than 15 and 10, I replaced them with correct values and combined these two columns to form a new rating column. In the final staged, I saved the predictions dataframe to a new csv file, whereas for the twitter archive and selected tweets, I joined them into a single master dataframe and saved them to a csv file, ready for analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('da_nanodegree': conda)",
   "language": "python",
   "name": "python37664bitdananodegreeconda551fcb2ecfa842b7a09bf3cc4a8f59b7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
